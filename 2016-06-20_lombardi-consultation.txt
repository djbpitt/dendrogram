Skype Consultation with Tom Lombardi, 2016-06-20

To validate output it is common to use bootstrapping. Randomly shuffle sequences and rerun to see how often the same relationships show up. If they recur regardless of the sequencing, they're likely to be useful. Since we're working with dissimilarities, we'd have to change basis for calculating distances; bootstrapping is more common in genomic type studies. "It would take a lot of work to figure out how to apply the model to your data", but it might be useful, although it's hard to come up with an appropriate model of randomness. This is also time-consuming, and may not be justified by the payoff.

As an alternative, perhaps validate on known cases. We could create a synthetic data set to test whether the metric does what we expect it to do in these theoretical cases, e.g., two highly similar short mss. Can also be used to explain the metric, which validation based on randomness doesn't explain how it works. This might be the best approach for deciding whether to keep the current metric or adjust it.

Dealing with varying length doesn't have a clear answer. This problem combines three things that aren't often combined, which makes it difficult to find reference materials: 1) is that sequence matters; 2) the distances are non-linear (longer sequences count for more at a greater-than-linear rate); 3) whether it's likely to happen by chance matters. There are methods that address two of these, but we don't know of any that deal with all three. There are similar problems in genomic studies, but they generally compare very large things, so the scaling problems aren't the same as here. This means that we need to think about how to normalize the data, and we've tried most of the strategies, but there isn't a metric that perfectly capture these data, so if we come up with something, we'll have to invent it. It's possible that no metric will adequately capture all of the salient properties. One of the challenges is that the distance metric is trying to summarize more than one thing at the same time. This is an original problem, and therefore worth addressing.

Some ideas:

Some data sets are evaluated according to a gravity model. Gravity models are non-linear (the closer two things are, the stronger the reaction; this is common in trade, where you're more likely to trade with someone close to you than someone further away, and in a non-linear way); they aren't that different from what we're already doing, but they might nonetheless merit examination. Strong on non-linear aspects of the data; may have other properties (needs a closer look). 

HMM appeals because we can define all known transition states (which texts precede or follow which others), and we could then use an edit-distance approach, but use the known probability for a particular substitution in the calculation. This is "hopelessly empirical" in that if you add new texts, everything can change. Bioinformatics uses different transition state models for different organisms; is there a substitution model that would address which substitutions are most common. This requires domain knowledge. 

Interrater agreement: To the extent that the text can be interpreted as "this is what I want to copy next" (that is, votes), there's a sort of ranking of texts. Interrater agreement would look at the extent to which two texts agree in what they consider important. The measure is called Cohen's kappa, and it reports the agreement of two raters in way that incorporates how likely the agreement is to arise by chance. One problem for interrater agreement with my data is length; it's more common in surveys, where everyone answers all of the questions, so it doesn't respond to missing data or alternate lengths. On the other hand, it has the how-likely-it-is-to-arise-by-chance element. Some have adapted it to missing data, but that isn't necessarily satisfying. Does some of what I want, but not really enough.

Differing lengths are handled by Levenshtein distance and others. These respect sequence in that they accommodate substitution, but the substitutions are equally valued in a way that doesn't capture the operative concept of sequence here. There would be some loss, and the loss is the sort that probably matters. HMM is an update of Levenshtein in that different substitutions have different weights. Not quick, though, but might be most responsive to these problems. The states would be based on my data, so it would be directed in the ways I care about. Doesn't need a lot of data, since it just uses the observed probabilities to calculate the part of the distance that is difficult to calculate otherwise. Doesn't have predictive value, but provides a basis for adjusting the distance metric. Run against the whole data set; no separation of training and test sets, which would be problematic for this data set.

Association rules are a weird approach, but it appeals here because it can use multiple measures of interest on the same data, many of which factor in how likely it is that we see this at random. R package is arules (check the tutorials); for my data, the trick is converting the data into something it can interpret. This is what Tom used to infer prestige in networks of saints in his Lausanne presentation. Direction isn't clear at face value (e.g., Anthony appears only with Francis but Francis can appear without Anthony), and this method can determine that directionality. May be useful, but might also be a distraction. Easier to work with than HMM, which is "a long haul", and will take a lot of time, so alternatives might be better. Tom prepared an on-line introduction to association rules for his students at http://cis241.washjeff-cis.net/October_1_2015.html; see also the https://cran.r-project.org/web/packages/arules/vignettes/arules.pdf vignette, which will come down with the package on installation. 

Association rules could help identify the likelihood that sequences of texts might occur at random. Translating that into a dendrogram is another step; association rules are more useful for understanding the transitions in the text than about supporting the visualizations. It might, though, help with validation. Construct a matrix that's 157 rows (mss) and 1522 columns (texts, boolean); association rules will identify the rules that probably didn't occur by chance. Some metrics are directed, e.g., confidence metric, which gives dependent probabilities. This isn't the same as sequence, but it does nonetheless wind up recognizing groups of texts that travel together. It finds as many relationships as it can find: one-to-one, one-to-many, many-to-many, etc. It doesn't respect sequences, but because it looks at the entire corpus at once, it does determine the likelihood of cooccurrence. E.g., if A always implies B, the confidence in that rule is 1, but the converse could be lower if B can occur without A. If it finds the same rules repeatedly, they might be related; it doesn't find the sequences, but I might be able to sequence the rules. Think of this as a validation method. It's less expensive in time commitment than HMM. 

What's hard about association rules is that it produces a lot of them, which then have to be queried to filter out the ones that matter. What Tom did was to recognize that the confidence rules create links between texts (in his case, saints); these could be graphed and then filtered with a confidence threshold (e.g., rules with 0 confidence, i.e., those never occur together, so filter those out). There is a network interpretation of association rules; no other obvious visualization. 

There is a different visualization comparable to the plectogram, but weaker for my purposes. If you plot each text in two mss on X and Y axes, if the two texts are identical, they match on the diagonal. If not, diagonal lines show up off the diagonal only where the texts match. Useful for finding matches quickly in sparse data. Called "dot plot"; R package is http://www.inside-r.org/packages/cran/seqinr/docs/dotPlot (and probably others). See examples at https://en.wikipedia.org/wiki/Dot_plot_%28bioinformatics%29. Can show only two at a time (unlike plectogram).

One use of HMM as a reference point might be not using it to modify metric (would take a long time), but calculate probabilities and display them as a heat map for researchers to see probability of transitions from one text to another. Colors for each cell are linked to numeric data in the cell; may have two or three colors (if probabilities range from 0 to 1, shade from white to red). Feed in sample sequence that represents the ordering of the texts in each ms, and it does it from there. R package is "markovchain". Can paste all texts together and treat them as one giant string (add a delimiter between them to determine probability of coming first or last). Result is 1500 x 1500, each cell is a transition state colored by the probability. The ordering may be arbitrary. It would be necessary to zoom in to make sense of it. Expect to see all zeroes down the diagonal, but otherwise there isn't much to see because the sequence is arbitrary and it would be necessary to zoom to see anything. You can resequence it like any matrix, but may not repay the effort. Might be more of a distraction than it's worth, but Tom will send some materials anyway.

Which clustering method? Agglomerative coefficient of everything is 0.2 except Ward's a 0.7. Ward's increases the distance so the heights are higher; in Ward's they're all at the 10 level because it squares a lot of stuff. Ward's makes things that are close look much closer, the height is more than double the height of any other method, so this seems to be giving the best results. If what you want is distance between clusters, Ward does more of this than the others, but that only makes sense if the clusters are meaningful. If they don't validate well, it's an artifact. Ward takes distance of every object to every other in both clusters. The others take max, min, or avg, but Ward's captures them all, which increases the number. One reason to be wary is that it's more susceptible to outliers, but that may be a Good Thing with this data set. Why Ward is a problem: Ward's minimum variance method has a lot of variants (most noticeably, squared vs raw distances), so we need to look at which one agnes() is using, which is sum of squares, which pushes the values up. The Ward's output is what Tom's output usually looks like, but it isn't clear whether that's useful for mss, and the fine detail in complete may be more useful. E.g., do we want to see a lot of small clusters, a fine-grained detail different from the large-scale grouping in Ward's? How about validating each of these approaches to see what each brings out? Ward's cares about large-scale grouping, which is what Tom needs with his data, but it may not be what a philologist cares about. Insofar as these are mathematical techniques operating on the data, none is necessarily more valid than the others, and they all require external validation. It may be that each model reveals something meaningful that the others don't.

If we look at complete, it's better at separating the smaller groups, while Ward's is better at the larger groups. Or, more precisely, the Ward visualization emphasizes the distances between larger groups, while with complete it's between larger groups. Ward's also separates smaller groups (at a lower height), but not in as clearly differentiated a way as complete.

Other ways to represent trees might also be helpful. http://washjeff-cis.net/cis241/phylo2.html; see also https://rpubs.com/gaston/dendrograms. Some may be more useful at some resolutions than others. Complete forms fewer clusters than Ward, but to the extent to which it forms clusters, how similar are they? Tanglegram visualization? https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html#tanglegram.

At this point only domain expertise can validate. There's no mathematical method. Bootstrapping might be worth keeping in mind (Tom will check whether there's a straightforward way to do it), but even after that, we'll need external validation. The distance metric can bake in biases, so although mathematical validation is independent at a certain level from expectation, the biases can enter at an earlier stage with distance metrics because there are no neutral distance metrics, just like there are no neutral dendrograms. How to deal with this? What are the goals of the visualizations? If it's to check whether what we're doing is true, that is, to challenge the interpretations, pick the most surprising trees. That's the essence of bootstrap: can we find any trees that don't do X? If certain sequences appear everywhere, it should be possible to generate a tree without them, but if it's relatively easy to generate the lesser ones, the pairings are more likely to be random. 

David: Send Tom Gribble FS article: confirmation of what we knew before, but also something new. What data mining is about is finding stuff we couldn't find without reading a lot of text, so we want to extend, and not just confirm, the vision. Tom works a lot in interdisciplinary computing, and a common complaint is "this only tells me what I already knew". Not only is that useful, but there's more. Jockers (in a footnote in his Macroanalysis book): a climate scientist with more confirmatory data doesn't say "we already knew that, so what use is your data?" Why in the humanities doesn't new evidence interest people? It's difficult to validate ideas in the humanities; don't we want as much evidence as possible to test whether something is true? Why only in the humanities is more evidence not interesting? The opposite is happening in the sciences, based on failures in late stages of drug trials, concerning evidence-based or empirically based approaches and their reliance on quantitative methods. "Where have all the case reports gone?" Medicine has become quantitative and evidence-based to an extent that may be reaching the limits of its usefulness, and that could be balanced with qualitative methods. There is now a flowering of statistical thinking that is challenging the status quo. 

There's a movement in computing education to make everything empirical and quantitatively driven. This is problematic; in the upcoming book about computing education Tom argues that a lot of education research needs to be like clinical practice. For example, we don't care about the average student when we teach; we care about the nuanced individual problems of individual students. What we learn from large-scale studies isn't of much use to us as educational practitioners. See Willful Ignorance, The New Statistics (null hypothesis significance testing has philosophical problems). This is unpopular in some circles, but it isn't anything that hasn't been said.

Where to cut? No good answer? agnes() exposes the heights, so we can cut at the greatest gap, but no procedure we can design will improve on human judgment. Perhaps make that part of the interactive nature, letting the user cut where necessary. Does the midpoint in the heights have a special value? It's more likely to with certain distributions. Drawing the line at 15 in Ward looks good, but drawing it at 6 in complete looks odd. Perhaps some combination of distance and the agglomerative coefficient. Perhaps the median is more useful when the agglomerative coefficient is higher? Average is more nuanced; because complete uses the max value, it flattens everything. Complete makes the small clusters look closer together than they really are; it smooths out a lot of the nuance that we would see in the distances with almost any other metric.

Action items:

Tom

1. Send references to Jockers footnote about how confirmation is devalued in the humanities.

2. Send script for putting together HMM chain and visualize with histogram.

3. Will research bootstrap validation, but this might take some time because of other commitments.

4. Will mention some of these issues at Keystone as a way of introducing to digital humanists things they might not usually see, with an eye toward trends that might impact their work.

David 

1. Send Tom article about micro-miscellanies.

2. External, human validation of clustering methods. Done: Anisava favors complete and Ward; both are now available on the site.

3. Look into arules. See http://cis241.washjeff-cis.net/October_1_2015.html and https://cran.r-project.org/web/packages/arules/vignettes/arules.pdf.

4. Look into qgraph.

5. Look into dotplot visualization. See http://www.inside-r.org/packages/cran/seqinr/docs/dotPlot.

6. Look into tanglegram to compare complete and Ward methods. https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html#tanglegram. Also explore alternatives to traditional dendragram at http://washjeff-cis.net/cis241/phylo2.html; see also https://rpubs.com/gaston/dendrograms.

7. Look into gravity model (may need more information).

8. Look into HMM for validation. See https://cran.rstudio.com/web/packages/markovchain/index.html and heat map visualization, although it may be difficult to create legible results with this much data.